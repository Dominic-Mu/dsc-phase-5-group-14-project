{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Health Assessment: Leveraging DHS Data for Targeted Interventions in Kenya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: [Alpha Guya](mailto:alpha.guya@student.moringaschool.com), [Ben Ochoro](mailto:ben.ochoro@student.moringaschool.com), [Caleb Ochieng](mailto:caleb.ochieng@student.moringaschool.com), [Christine Mukiri](mailto:christine.mukiri@student.moringaschool.com), [Dominic Muli](mailto:dominic.muli@student.moringaschool.com), [Frank Mandele](mailto:frank.mandele@student.moringaschool.com), [Jacquiline Tulinye](mailto:jacquiline.tulinye@student.moringaschool.com) and [Lesley Wanjiku](mailto:lesley.wanjiku@student.moringaschool.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0) Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project focuses on using machine learning techniques and data sourced from the Demographic and Health Surveys (DHS) program to generate predictive models aimed at evaluating individual and household health risks in Kenya. By analyzing various set of demographic, socio-economic, and health-related indicators, we target to develop reliable predictive models capable of estimating the likelihood of malnutrition, disease prevalence, and various health risks within certain communities. The goal is to provide users such as public health officials with targeted insights. This will enable more effective allotment of resources and interventions. This proactive approach is geared to optimize the impact of health initiatives, allowing for the prioritization and customization of interventions to at risk populations, ultimately contributing to the improvement of health outcomes in Kenya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite existing health interventions, Kenya encounters difficulties in effectively targeting resources and interventions. This will help to address individual and household health risks, including malnutrition, diseases, and other health concerns. This fault highlights the need for a predictive and targeted approach to allocate resources and interventions more effectively. Leveraging machine learning models built upon Demographic and Health Surveys (DHS) data, the project aims to develop predictive models capable of assessing the likelihood of malnutrition, disease prevalence, and health risks based on individual and household characteristics. By accurately identifying at-risk populations, this solution seeks to empower decision-makers and public health officials to allocate resources on need basis, ultimately increasing the impact of health interventions and improving overall health outcomes in Kenya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on the data provided by DHS(Demographic and Health Surveys) , some of the objectives include:\n",
    "\n",
    "* To analyze trends in health indicators over time.\n",
    "\n",
    "* To predict Health Risks based on individual and household characteristics. \n",
    "\n",
    "* To find the relationship between the most common diseases and the demographic.\n",
    "\n",
    "*  To Build predictive models to estimate health outcomes based on various demographic and socio-economic factors.\n",
    "\n",
    "*  To identify regional variations in health indicators.\n",
    "\n",
    "* To Identify factors contributing to changes in health outcomes.\n",
    "\n",
    "* To Conduct comprehensive feature engineering to extract relevant features from DHS data, considering demographic, socio-economic, and health-related variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Deployment and Usability\n",
    "\n",
    "Deploy an accessible API interface for stakeholders to input data and receive health risk predictions based on the developed models.\n",
    "\n",
    "### Recommendations and Conclusion\n",
    "* Targeted Intervention Recommendations:\n",
    "\n",
    "Utilize model predictions to generate targeted recommendations for health interventions and resource allocation in specific Kenyan communities.\n",
    "\n",
    "* Impact Assessment and Validation:\n",
    "\n",
    "Assess the real-world impact of model-guided interventions by monitoring and evaluating changes in health outcomes in targeted Kenyan populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Metric of Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Achieve a predictive accuracy of at least 90% on unseen validation data.\n",
    "* Identify and utilize the top 10 most influential features contributing to the models' predictive power.\n",
    "* Generate clear and interpretable explanations for at least 70% of model predictions.\n",
    "* Create a prioritized list of actionable recommendations based on identified health risks for at least 100 of communities.\n",
    "* Ensure an API uptime of at least 90% and gather feedback on usability for further improvements.\n",
    "* Measure the effectiveness of interventions by observing changes in health indicators, aiming for improvements in at least 80% of targeted communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Data Relevance and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data available is relevant for the intended analysis and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0) Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project is obtained from the [DHS Program website](https://dhsprogram.com/data/dataset/Kenya_Standard-DHS_2022.cfm?flag=0).\n",
    "The encoding for this dataset is explained [here](./Recode7_DHS_10Sep2018_DHSG4.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #installations\n",
    "#%pip install requests\n",
    "# %pip install pyreadstat\n",
    "#%pip install --upgrade openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import requests, json\n",
    "import urllib\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3) Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: we will work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading downloaded relevant data\n",
    "# df, meta = pyreadstat.read_sav(\"KEHR8BFL.SAV\")\n",
    "df_2, meta_2 = pyreadstat.read_sav(\"KEHR81FL.SAV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renaming columns in df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving column names into an Excel file\n",
    "\n",
    "# # Getting the column names\n",
    "# column_names = df.columns\n",
    "\n",
    "# # Creating a DataFrame with a single column containing the column names\n",
    "# column_names_df = pd.DataFrame(column_names, columns=[\"Column Names\"])\n",
    "\n",
    "# # Specifying the Excel file path\n",
    "# excel_file_path = 'column_names.xlsx'\n",
    "\n",
    "# # Writing the DataFrame to the Excel file\n",
    "# column_names_df.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting column names to labels dictionary to a DataFrame\n",
    "# labels_df = pd.DataFrame(list(meta.column_names_to_labels.items()), columns=['Column Name', 'Label'])\n",
    "\n",
    "# # Saving the DataFrame to an Excel file\n",
    "# excel_file_path = 'column_names_to_labels_1.xlsx'\n",
    "# labels_df.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting Coded column names into readable column names\n",
    "\n",
    "# # Loading the Excel file with the column names into a Pandas DataFrame\n",
    "# excel_file_path = 'updated_column_names_dictionary.xlsx'\n",
    "# df_excel = pd.read_excel(excel_file_path, sheet_name='Sheet1')\n",
    "\n",
    "# # Displaying the original DataFrame with the column headers\n",
    "# print(\"Original Excel DataFrame:\")\n",
    "# print(df_excel)\n",
    "\n",
    "# # Replacing the column headers using a for loop\n",
    "# for old_header, new_header in zip(df.columns, df_excel['Label Names']):\n",
    "#     df.rename(columns={old_header: new_header}, inplace=True)\n",
    "\n",
    "# # Displaying the DataFrame with the updated column headers\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renaming columns in df_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_columns(df, prefix, suffixes, combined_column):\n",
    "    # Extracting columns with the specified prefix and suffixes\n",
    "    relevant_columns = [col for col in df.columns if col.startswith(prefix) and any(col.endswith(suffix) for suffix in suffixes)]\n",
    "\n",
    "    # Creating a new column 'combined_column' with the highest value for each row\n",
    "    df[combined_column] = df[relevant_columns].max(axis=1)\n",
    "\n",
    "    # Dropping the original columns\n",
    "    df.drop(relevant_columns, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def filter_and_exclude_prefix(df, prefix, exclude_first_n=6):\n",
    "    filtered_columns = [col for col in df.columns if col.startswith(prefix)]\n",
    "    modified_columns = [col[exclude_first_n:] for col in filtered_columns]\n",
    "    return modified_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['HV120',\n",
    "           'HC53', 'HC55','HC56','HC57',\n",
    "           'SB115A','SB115B','SB115C', 'SB115D','SB115E','SB115F','SB115G','SB115H', 'SB119','SB122',\n",
    "           'HML5','HML6','HML10', 'HML11','HML12', 'HML18', 'HML19',\n",
    "           'HML32','HML32A', 'HML32B', 'HML32C','HML32D', 'HML32E', 'HML32F', 'HML32G', 'HML33', 'HML34', 'HML35']\n",
    "\n",
    "for i in columns:\n",
    "    df_2_suffixes = filter_and_exclude_prefix(df_2, f'{i}$')\n",
    "    # print(df_2_suffixes)\n",
    "    collapse_columns(df_2, f'{i}$', df_2_suffixes, f'{i}_combined')\n",
    "    \n",
    "    # print(malaria_df[f'{i}_combined'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving column names into an Excel file\n",
    "\n",
    "# Getting the column names\n",
    "column_names_2 = df_2.columns\n",
    "\n",
    "# Creating a DataFrame with a single column containing the column names\n",
    "column_names_df_2 = pd.DataFrame(column_names_2, columns=[\"Column Names 2\"])\n",
    "\n",
    "# Specifying the Excel file path\n",
    "excel_file_path_2 = 'column_names_to_labels_2.xlsx'\n",
    "\n",
    "# Writing the DataFrame to the Excel file\n",
    "column_names_df_2.to_excel(excel_file_path_2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting column names to labels dictionary to a DataFrame\n",
    "labels_df_2 = pd.DataFrame(list(meta_2.column_names_to_labels.items()), columns=['Column Name 2', 'Label 2'])\n",
    "\n",
    "# Saving the DataFrame to an Excel file\n",
    "excel_file_path_2 = 'column_names_to_labels_2.xlsx'\n",
    "labels_df_2.to_excel(excel_file_path_2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Coded column names into readable column names\n",
    "\n",
    "# Loading the Excel file with the column names into a Pandas DataFrame\n",
    "excel_file_path_2 = 'column_names_dictionary_2.xlsx'\n",
    "df_excel_2 = pd.read_excel(excel_file_path_2, sheet_name='Sheet1')\n",
    "\n",
    "# Displaying the original DataFrame with the column headers\n",
    "print(\"Original Excel DataFrame:\")\n",
    "print(df_excel_2)\n",
    "\n",
    "# Replacing the column headers using a for loop\n",
    "for old_header, new_header in zip(df_2.columns, df_excel_2['Label Names']):\n",
    "    df_2.rename(columns={old_header: new_header}, inplace=True)\n",
    "\n",
    "# Displaying the DataFrame with the updated column headers\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_2.columns[:1955]:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2['childs_age_in_months_40'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "column_stats = []\n",
    "\n",
    "for column in df_2.columns:\n",
    "    nan_sum = df_2[column].isna().sum()\n",
    "    nan_percentage = (nan_sum / len(df_2)) * 100\n",
    "    column_stats.append((column, nan_sum, nan_percentage))\n",
    "\n",
    "# Create a DataFrame from the list of tuples\n",
    "column_stats_df = pd.DataFrame(column_stats, columns=['Column Name', 'NaN Sum', 'NaN Percentage'])\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the DataFrame\n",
    "column_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "\n",
    "# List of column names to drop\n",
    "columns_to_drop = [\"respondent's_ line_number \", \"sample_weight\", \"month_of_the_household_interview\", \"month_date_of_household_interview\", \n",
    "                  \"day_date_of_household_interview\", \"result_of_household_interview\", \"day_of_the_household_interview\", \n",
    "                  \"number_of_visits\", \"interviewer_id\", \"ever_married\", \"primary_sampling_unit\", \"strata_for_sampling_errors\", \"sample_design\",\n",
    "                  \"male_survey\", \"sample_weight_male\", \"field_supervisor\", \"children_under_five\", \"cluster_altitude\", \"women_height_weight_measured\", \n",
    "                  \"questionnaire_language\", \"interview_language\", \"respodent_native_language\", \"translator_used\", \"interview_start_time\", \n",
    "                  \"interview_end_time\", \"interview_length\", \"time_to_water_source\", \"household_relationship_structure\", \"line_number_of_head_of_household\", \n",
    "                  \"type_toilet_facility\", \"toilet_location\", \"member_has_bank_account\", \"wealth_index_factor_urban_rural\",\"wealth_index_factor\",\n",
    "                  \"line_number_1\", \"line_number_2\", \"line_number_21\", \"line_number_23\", \"line_number_24\", \"rshp_to_head_1\", \"rshp_to_head_2\", \n",
    "                  \"relationship_to_head_23\", \"relationship_to_head_24\", \"usual_resident_1\", \"usual_resident_2\", \"stayed_last_night_1\", \"stayed_last_night_2\", \n",
    "                  \"stayed_last_night_3\", \"stayed_last_night_4\", \"stayed_last_night_5\", \"stayed_last_night_6\", \"stayed_last_night_7\", \"stayed_last_night_8\", \n",
    "                  \"sex_of_household_member_5\", \"sex_of_household_member_6\", \"sex_of_household_member_7\", \"sex_of_household_member_8\", \"sex_of_household_member_9\", \n",
    "                  \"female_int_eligibility_1\", \"female_int_eligibility_2\", \"child_hwh_eligibility_1\", \n",
    "                  \"child_hwh_eligibility_2\", \"index_to_household_schedule_hmhidx_1\", \"index_to_household_schedule_hmhidx_2\", \"net_design_no_1\", \"net_design_no_2\", \n",
    "                  \"flag_age_1\", \"flag_age_2\", \"food_prep_place\", \"household_has_separate_room_used_as_kitchen\", \"owns_horses_donkeys\", \"usual_resident_1\", \"usual_resident_2\", \n",
    "                  \"usual_resident_3\", \"usual_resident_4\", \"index_to_household_schedule_7\", \"eligibility_for_female_interview_9\", \"eligibility_for_female_interview_10\", \n",
    "                  \"eligibility_for_female_interview_11\", \"eligibility_for_female_interview_12\", \"Date_measured_day_7\", \"Date_measured_month_7\", \"Date_measured_year_7\", \n",
    "                  \"century_day_code_of_measurement_7\", \"fieldworker_measurer_code_7\", \"sex_7\", \"month_of_birth_7\", \"year_of birth_7\", \"cmc_date_of_birth_7\", \"century_day_code_of_birth_7\", \n",
    "                  \"completeleness_of_hc32_info_7\", \"line_number_of_parent_caretaker_7\", \"read_consent_statement_hemoglobin_7\", \"hemoglobin_level_g_dl_7\", \n",
    "                  \"hemoglobin_level_adjusted_for_altitude_g_dl_7\", \"mothers_heighest_educational_level_final_report_7\", \"childs_age_in_months_country_specific_7\", \"childs_age_in_months_country_specific_8\",\n",
    "                   \"childs_age_in_months_country_specific_9\", \"fieldworker_number_for_malaria_medicine_7\", \"fieldworker_number_for_malaria_medicine_8\", \"fieldworker_number_for_malaria_medicine_9\",\n",
    "                   \"day_of_data_collection_7\", \"day_of_data_collection_8\", \"day_of_data_collection_9\", \"month_of_data_collection_7\", \"month_of_data_collection_8\", \"month_of_data_collection_9\", \n",
    "                   \"year_of_data_collection_7\", \"year_of_data_collection_8\", \"mosquito_bed_net_designation_number_1\", \"mosquito_bed_net_designation_number_2\", \"na_net_treated_with_insecticide_when_bought_1\", \n",
    "                   \"na_net_treated_with_insecticide_when_bought_2\", \"na_net_treated_with_insecticide_when_bought_3\", \"na_net_treated_since_received_1\", \"na_net_treated_since_received_2\", \"na_net_treated_since_received_3\",\n",
    "                   \"line_number_of_person_slept_in_net_d_1\", \"line_number_of_person_slept_in_net_d_2\"]  \n",
    "\n",
    "# Drop the specified columns\n",
    "df_2_cleaned_dropped = df_2.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_2_cleaned_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the threshold for 80% empty columns\n",
    " #threshold = 0.5 * len(df_2)\n",
    "\n",
    "# # Dropping columns with 80% or more empty values\n",
    " #combined_df_cleaned = df_2.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "# # Displaying the cleaned DataFrame\n",
    " #combined_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collapse_columns(df_2, prefixes, suffixes, combined_column_prefix):\n",
    "#     for prefix in prefixes:\n",
    "#         # Extracting columns with the specified prefix and suffixes\n",
    "#         relevant_columns = [col for col in df_2.columns if col.startswith(prefix) and any(col.endswith(suffix) for suffix in suffixes)]\n",
    "\n",
    "#         # Creating a new column 'combined_column' with the highest value for each row\n",
    "#         combined_column_name = f'{combined_column_prefix}_{prefix}'\n",
    "#         df_2[combined_column_name] = df_2[relevant_columns].max(axis=1)\n",
    "\n",
    "#         # Dropping the original columns\n",
    "#         df_2.drop(relevant_columns, axis=1, inplace=True)\n",
    "\n",
    "#     return df_2\n",
    "\n",
    "# # Specify the prefixes and suffixes for the columns you want to collapse\n",
    "# prefixes_to_collapse = ['HML32$', 'SH305E$', 'HML35$', 'SB115B$', 'SB115F$', 'HML8$', 'SH130$', 'SB115A$', 'SB115E$', 'SB119$',\n",
    "#                        'HML33$', 'SB115C$', 'SB115G$', 'SB122$', 'HC57$', 'SB115D$', 'SB115H$', 'HML5$']\n",
    "# suffixes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14',\n",
    "#             '15', '16', '17', '18', '19', '20', '21', '22', '23', '24']\n",
    "\n",
    "# # Call the function for collapsing the specified columns\n",
    "# collapsed_df_1 = collapse_columns(df_2, prefixes_to_collapse, suffixes, 'collapsed')\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# collapsed_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# combined_df = pd.merge(df, collapsed_df_1, on='Household_ID', how='inner')\n",
    "# combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking percentage of missing values\n",
    "# def missing_values_summary(combined_df):\n",
    "#     \"\"\"\n",
    "#     Generate a summary of missing values for each column in a DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: pandas DataFrame\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame containing columns with NaN values and their percentages\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Checking percentage of missing values\n",
    "#     nan_info = combined_df.isna().sum()\n",
    "#     nan_percentage = (nan_info / len(combined_df)) * 100\n",
    "\n",
    "#     # Creating a DataFrame with columns and their NaN percentages\n",
    "#     nan_df = pd.DataFrame({'Column': nan_info.index, 'NaN Count': nan_info.values, 'NaN Percentage': nan_percentage.values})\n",
    "\n",
    "#     # Filtering columns with NaN values\n",
    "#     columns_with_nan = nan_df[nan_df['NaN Count'] > 0]\n",
    "\n",
    "#     return columns_with_nan\n",
    "\n",
    "\n",
    "# # Calling the function on df_cleaned\n",
    "# result = missing_values_summary(combined_df)\n",
    "# result.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replacing empty(missing values) with NaN\n",
    "# combined_df.replace(' ',np.nan, inplace=True)\n",
    "# combined_df.replace(\"\",np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the threshold for 80% empty columns\n",
    "# threshold = 0.8 * len(combined_df)\n",
    "\n",
    "# # Dropping columns with 80% or more empty values\n",
    "# combined_df_cleaned = combined_df.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "# # Displaying the cleaned DataFrame\n",
    "# combined_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for col in combined_df_cleaned.columns[:343]:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = missing_values_summary(combined_df_cleaned)\n",
    "# result.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df_cleaned['collapsed_HML32$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replacing empty(missing values) with NaN\n",
    "# df.replace(' ',np.nan, inplace=True)\n",
    "# df.replace(\"\",np.nan, inplace=True)\n",
    "# df_2.replace(' ',np.nan, inplace=True)\n",
    "# df_2.replace(\"\",np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the threshold for 80% empty columns\n",
    "# threshold = 0.8 * len(df)\n",
    "\n",
    "# # Dropping columns with 80% or more empty values\n",
    "# df_cleaned = df.dropna(axis=1, thresh=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the threshold for 80% empty columns\n",
    "# threshold_1 = 0.8 * len(df_2)\n",
    "\n",
    "# # Dropping columns with 80% or more empty values\n",
    "# df_2_cleaned = df_2.dropna(axis=1, thresh=threshold_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.columns == df_2_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_list = ['SH119', 'SH130$1', 'SH130$2', 'SH130$3', 'SH130$4', 'SH130$5', 'SH130$6', 'SH130$7', 'HML32$01', 'HML32$02', 'HML32$03', \n",
    "#               'HML32$04', 'HML32$05', 'HML32$06', 'HML32$07', 'HML32$08', 'HML32$09', 'HML32$10', 'HML32$11', 'HML32$12', 'HML32$13', \n",
    "#               'HML32$14', 'HML32$15', 'HML32$16', 'HML32$17', 'HML32$18', 'HML32$19', 'HML32$20', 'HML32$21', 'HML32$22', 'HML32$23', \n",
    "#               'HML32$24', 'HML35$01', 'HML35$02', 'HML35$03', 'HML35$04', 'HML35$05', 'HML35$06', 'HML35$07', 'HML35$08', 'HML35$09', \n",
    "#               'HML35$10', 'HML35$11', 'HML35$12', 'HML35$13', 'HML35$14', 'HML35$15', 'HML35$16', 'HML35$17', 'HML35$18', 'HML35$19', \n",
    "#               'HML35$20', 'HML35$21', 'HML35$22', 'HML35$23', 'HML35$24', 'HML33$01', 'HML33$02', 'HML33$03', 'HML33$04', 'HML33$05', \n",
    "#               'HML33$06', 'HML33$07', 'HML33$08', 'HML33$09', 'HML33$10', 'HML33$11', 'HML33$12', 'HML33$13', 'HML33$14', 'HML33$15', \n",
    "#               'HML33$16', 'HML33$17', 'HML33$18', 'HML33$19', 'HML33$20', 'HML33$21', 'HML33$22', 'HML33$23', 'HML33$24', 'HC57$01', \n",
    "#               'HC57$02', 'HC57$03', 'HC57$04', 'HC57$05', 'HC57$06', 'HC57$07', 'HC57$08', 'HC57$09', 'HC57$10', 'HC57$11', 'HC57$12', \n",
    "#               'HC57$13', 'HC57$14', 'HC57$15', 'HC57$16', 'HC57$17', 'HC57$18', 'HC57$19', 'HC57$20', 'SB115A$01', 'SB115A$02', \n",
    "#               'SB115A$03', 'SB115A$04', 'SB115A$05', 'SB115A$06', 'SB115A$07', 'SB115A$08', 'SB115A$09', 'SB115A$10', 'SB115A$11', \n",
    "#               'SB115A$12', 'SB115A$13', 'SB115A$14', 'SB115A$15', 'SB115A$16', 'SB115A$17', 'SB115A$18', 'SB115A$19', 'SB115A$20', \n",
    "#               'SB115B$01', 'SB115B$02', 'SB115B$03', 'SB115B$04', 'SB115B$05', 'SB115B$06', 'SB115B$07', 'SB115B$08', 'SB115B$09', \n",
    "#               'SB115B$10', 'SB115B$11', 'SB115B$12', 'SB115B$13', 'SB115B$14', 'SB115B$15', 'SB115B$16', 'SB115B$17', 'SB115B$18', \n",
    "#               'SB115B$19', 'SB115B$20', 'SB115C$01', 'SB115C$02', 'SB115C$03', 'SB115C$04', 'SB115C$05', 'SB115C$06', 'SB115C$07', \n",
    "#               'SB115C$08', 'SB115C$09', 'SB115C$10', 'SB115C$11', 'SB115C$12', 'SB115C$13', 'SB115C$14', 'SB115C$15', 'SB115C$16', \n",
    "#               'SB115C$17', 'SB115C$18', 'SB115C$19', 'SB115C$20', 'SB115D$01', 'SB115D$02', 'SB115D$03', 'SB115D$04', 'SB115D$05', \n",
    "#               'SB115D$06', 'SB115D$07', 'SB115D$08', 'SB115D$09', 'SB115D$10', 'SB115D$11', 'SB115D$12', 'SB115D$13', 'SB115D$14', \n",
    "#               'SB115D$15', 'SB115D$16', 'SB115D$17', 'SB115D$18', 'SB115D$19', 'SB115D$20', 'SB115E$01', 'SB115E$02', 'SB115E$03', \n",
    "#               'SB115E$04', 'SB115E$05', 'SB115E$06', 'SB115E$07', 'SB115E$08', 'SB115E$09', 'SB115E$10', 'SB115E$11', 'SB115E$12', \n",
    "#               'SB115E$13', 'SB115E$14', 'SB115E$15', 'SB115E$16', 'SB115E$17', 'SB115E$18', 'SB115E$19', 'SB115E$20', 'SB115F$01', \n",
    "#               'SB115F$02', 'SB115F$03', 'SB115F$04', 'SB115F$05', 'SB115F$06','SB115F$07', 'SB115F$08', 'SB115F$09', 'SB115F$10', \n",
    "#               'SB115F$11', 'SB115F$12', 'SB115F$13', 'SB115F$14', 'SB115F$15', 'SB115F$16', 'SB115F$17', 'SB115F$18', 'SB115F$19', \n",
    "#               'SB115F$20', 'SB115G$01', 'SB115G$02', 'SB115G$03', 'SB115G$04', 'SB115G$05', 'SB115G$06', 'SB115G$07', 'SB115G$08', \n",
    "#               'SB115G$09', 'SB115G$10', 'SB115G$11', 'SB115G$12', 'SB115G$13', 'SB115G$14', 'SB115G$15', 'SB115G$16', 'SB115G$17', \n",
    "#               'SB115G$18', 'SB115G$19', 'SB115G$20', 'SB115H$01', 'SB115H$02', 'SB115H$03', 'SB115H$04', 'SB115H$05', 'SB115H$06', \n",
    "#               'SB115H$07', 'SB115H$08', 'SB115H$09', 'SB115H$10', 'SB115H$11', 'SB115H$12', 'SB115H$13', 'SB115H$14', 'SB115H$15', \n",
    "#               'SB115H$16', 'SB115H$17', 'SB115H$18', 'SB115H$19', 'SB115H$20', 'SB119$01', 'SB119$02', 'SB119$03', 'SB119$04', 'SB119$05', \n",
    "#               'SB119$06', 'SB119$07', 'SB119$08', 'SB119$09', 'SB119$10', 'SB119$11', 'SB119$12', 'SB119$13', 'SB119$14', 'SB119$15', \n",
    "#               'SB119$16', 'SB119$17', 'SB119$18', 'SB119$19', 'SB119$20', 'SB122$01', 'SB122$02', 'SB122$03', 'SB122$04', 'SB122$05', \n",
    "#               'SB122$06', 'SB122$07', 'SB122$08', 'SB122$09', 'SB122$10', 'SB122$11', 'SB122$12', 'SB122$13', 'SB122$14', 'SB122$15', \n",
    "#               'SB122$16', 'SB122$17', 'SB122$18', 'SB122$19', 'SB122$20', 'HML5$1', 'HML5$2', 'HML5$3', 'HML5$4', 'HML5$5', 'HML5$6', \n",
    "#               'HML5$7', 'HML8$1']\n",
    "\n",
    "# new_df_2 = df_2[column_list].copy()\n",
    "\n",
    "# new_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df_2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df_2['HML35$01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming 'df' is your DataFrame containing health test data\n",
    "# # Replace 'your_prefix' with the common prefix of the columns you want to collapse\n",
    "# prefix = 'your_prefix'\n",
    "\n",
    "# # Get a list of columns that have the specified prefix\n",
    "# columns_to_collapse = [col for col in df.columns if col.startswith(prefix)]\n",
    "\n",
    "# # Create a new column by concatenating values from columns with the specified prefix\n",
    "# df['collapsed_column'] = df[columns_to_collapse].astype(str).agg(','.join, axis=1)\n",
    "\n",
    "# # Drop the original columns if needed\n",
    "# df.drop(columns=columns_to_collapse, inplace=True)\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def collapse_columns(df, prefix, suffixes, combined_column):\n",
    "#     # Extract columns with the specified prefix and suffixes\n",
    "#     relevant_columns = [col for col in df.columns if col.startswith(prefix) and any(col.endswith(suffix) for suffix in suffixes)]\n",
    "\n",
    "#     # Create a new column 'combined_column' with the highest value for each row\n",
    "#     df[combined_column] = df[relevant_columns].max(axis=1)\n",
    "\n",
    "#     # Replace NaN values in the 'combined_column' with 0\n",
    "#     df[combined_column].fillna(0, inplace=True)\n",
    "\n",
    "#     # Drop the original columns\n",
    "#     df.drop(relevant_columns, axis=1, inplace=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming your DataFrame is named 'your_dataframe'\n",
    "# your_dataframe = pd.DataFrame({\n",
    "#     'HML32$01': [0, 1, 0],\n",
    "#     'HML32$02': [1, 0, 1],\n",
    "#     'OtherColumn': ['A', 'B', 'C']\n",
    "# })\n",
    "\n",
    "# collapsed_df = collapse_columns(your_dataframe, 'HML32$', ['01', '02'], 'HML32_combined')\n",
    "\n",
    "# collapsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking percentage of missing values\n",
    "# def missing_values_summary(df):\n",
    "#     \"\"\"\n",
    "#     Generate a summary of missing values for each column in a DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: pandas DataFrame\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame containing columns with NaN values and their percentages\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Checking percentage of missing values\n",
    "#     nan_info = df.isna().sum()\n",
    "#     nan_percentage = (nan_info / len(df)) * 100\n",
    "\n",
    "#     # Creating a DataFrame with columns and their NaN percentages\n",
    "#     nan_df = pd.DataFrame({'Column': nan_info.index, 'NaN Count': nan_info.values, 'NaN Percentage': nan_percentage.values})\n",
    "\n",
    "#     # Filtering columns with NaN values\n",
    "#     columns_with_nan = nan_df[nan_df['NaN Count'] > 0]\n",
    "\n",
    "#     return columns_with_nan\n",
    "\n",
    "\n",
    "# # Calling the function on df_cleaned\n",
    "# result = missing_values_summary(df_cleaned)\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking percentage of missing values on df_2_cleaned\n",
    "# result_2 = missing_values_summary(df_2_cleaned)\n",
    "# result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving column names into an Excel file\n",
    "\n",
    "# # Getting the column names\n",
    "# column_names = df_cleaned.columns\n",
    "\n",
    "# # Creating a DataFrame with a single column containing the column names\n",
    "# column_names_df = pd.DataFrame(column_names, columns=[\"Column Names\"])\n",
    "\n",
    "# # Specifying the Excel file path\n",
    "# excel_file_path = 'column_names.xlsx'\n",
    "\n",
    "# # Writing the DataFrame to the Excel file\n",
    "# column_names_df.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting column names to labels dictionary to a DataFrame\n",
    "# labels_df = pd.DataFrame(list(meta.column_names_to_labels.items()), columns=['Column Name', 'Label'])\n",
    "\n",
    "# # Saving the DataFrame to an Excel file\n",
    "# excel_file_path = 'column_names_to_labels.xlsx'\n",
    "# labels_df.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting Coded column names into readable column names\n",
    "\n",
    "# # Loading the Excel file with the column names into a Pandas DataFrame\n",
    "# excel_file_path = 'column_names_dictionary.xlsx'\n",
    "# df_excel = pd.read_excel(excel_file_path, sheet_name='Sheet1')\n",
    "\n",
    "# # Displaying the original DataFrame with the column headers\n",
    "# print(\"Original Excel DataFrame:\")\n",
    "# print(df_excel)\n",
    "\n",
    "# # Replacing the column headers using a for loop\n",
    "# for old_header, new_header in zip(df_cleaned.columns, df_excel['Label Names']):\n",
    "#     df_cleaned.rename(columns={old_header: new_header}, inplace=True)\n",
    "\n",
    "# # Displaying the DataFrame with the updated column headers\n",
    "# df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imputing the DataFrame \n",
    "\n",
    "# # Initializing  SimpleImputer\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# # Identifying numeric and non-numeric columns\n",
    "# numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "# non_numeric_cols = df_cleaned.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "\n",
    "# # Imputing numeric columns\n",
    "# numeric_imputer = SimpleImputer(strategy='mean')\n",
    "# df_cleaned[numeric_cols] = numeric_imputer.fit_transform(df_cleaned[numeric_cols])\n",
    "\n",
    "# # Imputing non-numeric columns\n",
    "# non_numeric_imputer = SimpleImputer(strategy='most_frequent')\n",
    "# df_cleaned[non_numeric_cols] = non_numeric_imputer.fit_transform(df_cleaned[non_numeric_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking percentage of missing values\n",
    "\n",
    "# result = missing_values_summary(df_cleaned)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing distributions of numerical features\n",
    "# for col in numeric_cols:\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.histplot(df[col].dropna(), kde=True)\n",
    "#     plt.title(f'Distribution of {col}')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize relationships between variables\n",
    "# sns.pairplot(df_cleaned)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation heatmap\n",
    "# correlation_matrix = df_cleaned.corr()\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model Interpretability and Explainability:\n",
    "\n",
    "Enhance model interpretability to provide actionable insights for decision-makers by employing techniques such as SHAP values or feature importance analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6) Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7) Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
